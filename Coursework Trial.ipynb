{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39581322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(999)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "#from tensorflow.keras.regularizers import L1\n",
    "#from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd561a05",
   "metadata": {},
   "source": [
    "### Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "026554bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel = pd.read_pickle('Data/returns_chars_panel.pkl') # all the stocks over all the time\n",
    "#file too large, download it into D disk.\n",
    "macro = pd.read_pickle('Data/macro_timeseries.pkl')   # macro indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b90acb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>excess_ret</th>\n",
       "      <th>ret</th>\n",
       "      <th>rfree</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>stdacc</th>\n",
       "      <th>stdcf</th>\n",
       "      <th>ms</th>\n",
       "      <th>baspread</th>\n",
       "      <th>ill</th>\n",
       "      <th>maxret</th>\n",
       "      <th>retvol</th>\n",
       "      <th>std_dolvol</th>\n",
       "      <th>std_turn</th>\n",
       "      <th>zerotrade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986-02-01</td>\n",
       "      <td>10000</td>\n",
       "      <td>-0.262610</td>\n",
       "      <td>-0.257143</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>-0.375440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572822</td>\n",
       "      <td>0.361538</td>\n",
       "      <td>0.938929</td>\n",
       "      <td>0.873640</td>\n",
       "      <td>0.399871</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>-0.177810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.360335</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>-0.496811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.417504</td>\n",
       "      <td>0.520205</td>\n",
       "      <td>-0.138898</td>\n",
       "      <td>0.328870</td>\n",
       "      <td>0.078782</td>\n",
       "      <td>-0.411202</td>\n",
       "      <td>0.130393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986-04-01</td>\n",
       "      <td>10000</td>\n",
       "      <td>-0.103717</td>\n",
       "      <td>-0.098592</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>-0.401783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.521182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091488</td>\n",
       "      <td>0.329401</td>\n",
       "      <td>0.813365</td>\n",
       "      <td>0.660035</td>\n",
       "      <td>0.142035</td>\n",
       "      <td>-0.131985</td>\n",
       "      <td>0.119017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1986-05-01</td>\n",
       "      <td>10000</td>\n",
       "      <td>-0.227831</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>-0.435735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.439391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.329949</td>\n",
       "      <td>0.438295</td>\n",
       "      <td>-0.683852</td>\n",
       "      <td>-0.759048</td>\n",
       "      <td>0.388029</td>\n",
       "      <td>-0.199555</td>\n",
       "      <td>0.135402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-06-01</td>\n",
       "      <td>10000</td>\n",
       "      <td>-0.009883</td>\n",
       "      <td>-0.005025</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>-0.534203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.499678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510140</td>\n",
       "      <td>0.610586</td>\n",
       "      <td>0.724257</td>\n",
       "      <td>0.608819</td>\n",
       "      <td>-0.128302</td>\n",
       "      <td>-0.025964</td>\n",
       "      <td>0.007879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  permno  excess_ret       ret     rfree     mvel1  beta  betasq  \\\n",
       "0 1986-02-01   10000   -0.262610 -0.257143  0.005467 -0.375440   0.0     0.0   \n",
       "1 1986-03-01   10000    0.360335  0.365385  0.005050 -0.496811   0.0     0.0   \n",
       "2 1986-04-01   10000   -0.103717 -0.098592  0.005125 -0.401783   0.0     0.0   \n",
       "3 1986-05-01   10000   -0.227831 -0.222656  0.005175 -0.435735   0.0     0.0   \n",
       "4 1986-06-01   10000   -0.009883 -0.005025  0.004858 -0.534203   0.0     0.0   \n",
       "\n",
       "   chmom    dolvol  ...  stdacc  stdcf   ms  baspread       ill    maxret  \\\n",
       "0    0.0  0.000000  ...     0.0    0.0  0.0  0.572822  0.361538  0.938929   \n",
       "1    0.0  0.000000  ...     0.0    0.0  0.0  0.417504  0.520205 -0.138898   \n",
       "2    0.0 -0.521182  ...     0.0    0.0  0.0  0.091488  0.329401  0.813365   \n",
       "3    0.0 -0.439391  ...     0.0    0.0  0.0  0.329949  0.438295 -0.683852   \n",
       "4    0.0 -0.499678  ...     0.0    0.0  0.0  0.510140  0.610586  0.724257   \n",
       "\n",
       "     retvol  std_dolvol  std_turn  zerotrade  \n",
       "0  0.873640    0.399871  0.120000  -0.177810  \n",
       "1  0.328870    0.078782 -0.411202   0.130393  \n",
       "2  0.660035    0.142035 -0.131985   0.119017  \n",
       "3 -0.759048    0.388029 -0.199555   0.135402  \n",
       "4  0.608819   -0.128302 -0.025964   0.007879  \n",
       "\n",
       "[5 rows x 99 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dp</th>\n",
       "      <th>ep</th>\n",
       "      <th>b/m</th>\n",
       "      <th>crsp_spvw</th>\n",
       "      <th>svar</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>dfy</th>\n",
       "      <th>dfr</th>\n",
       "      <th>ntis</th>\n",
       "      <th>infl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1926-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>1927-01-01</td>\n",
       "      <td>-2.973012</td>\n",
       "      <td>-2.386837</td>\n",
       "      <td>0.441476</td>\n",
       "      <td>0.026047</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>0.050876</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1927-02-01</td>\n",
       "      <td>-2.942374</td>\n",
       "      <td>-2.374773</td>\n",
       "      <td>0.443706</td>\n",
       "      <td>-0.002910</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.050824</td>\n",
       "      <td>-0.011299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>1927-03-01</td>\n",
       "      <td>-2.979535</td>\n",
       "      <td>-2.430353</td>\n",
       "      <td>0.428501</td>\n",
       "      <td>0.045522</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>0.051668</td>\n",
       "      <td>-0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>1927-04-01</td>\n",
       "      <td>-2.976535</td>\n",
       "      <td>-2.445079</td>\n",
       "      <td>0.469765</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>-0.0170</td>\n",
       "      <td>0.046357</td>\n",
       "      <td>-0.005747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date        dp        ep       b/m  crsp_spvw      svar     tbl  \\\n",
       "671 1926-12-01       NaN       NaN       NaN        NaN       NaN     NaN   \n",
       "672 1927-01-01 -2.973012 -2.386837  0.441476   0.026047  0.000465  0.0307   \n",
       "673 1927-02-01 -2.942374 -2.374773  0.443706  -0.002910  0.000470  0.0323   \n",
       "674 1927-03-01 -2.979535 -2.430353  0.428501   0.045522  0.000287  0.0329   \n",
       "675 1927-04-01 -2.976535 -2.445079  0.469765   0.007324  0.000924  0.0320   \n",
       "\n",
       "        tms     dfy     dfr      ntis      infl  \n",
       "671     NaN     NaN     NaN       NaN       NaN  \n",
       "672  0.0047  0.0100 -0.0022  0.050876  0.000000  \n",
       "673  0.0028  0.0095 -0.0019  0.050824 -0.011299  \n",
       "674  0.0018  0.0092 -0.0019  0.051668 -0.005714  \n",
       "675  0.0011  0.0092 -0.0170  0.046357 -0.005747  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(panel.head(), macro.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cbae45",
   "metadata": {},
   "source": [
    "Note: the above dates are not in a chronological order. 'panel' data starts from 1957-03."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33957ed9",
   "metadata": {},
   "source": [
    "- Group stock-level characteristics and macro variables together.\n",
    "- Define Features (X) and Targets (Y).<br> \n",
    "Note: the paper did not use macro variable 'crsp_spvw', 'dfr' and 'infl' so we will drop these out as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15b48d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3739449, 107)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine micro and macro data\n",
    "# In the paper, the variables [crsp_spvw, dfr, infl] are not used\n",
    "df = pd.merge(panel,macro.drop(columns=['crsp_spvw','dfr','infl']),on='date',how='left',suffixes=['','_macro'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c98cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the storing space\n",
    "del panel, macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de693f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29853"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['permno'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2aaee005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '11' '12' '13' '14' '15' '16' '17' '18' '19' '20' '21' '22' '23'\n",
      " '24' '25' '26' '27' '28' '29' '30' '31' '32' '33' '34' '35' '36' '37'\n",
      " '38' '39' '40' '41' '42' '43' '44' '45' '46' '47' '48' '49' '50' '51'\n",
      " '52' '53' '54' '55' '56' '57' '58' '59' '60' '61' '62' '63' '64' '65'\n",
      " '66' '67' '68' '69' '70' '71' '72' '73' '74' '75' '76' '77' '78' '79'\n",
      " '80' '81' '82' '83' '84' '85' '86' '87' '88' '89' '90' '91' '92' '93']\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "# there are 84 industrial codes (first two digits of the original codes) \n",
    "df['permno'] = df['permno'].astype('str').str[:2]\n",
    "\n",
    "inds = df['permno'].unique()\n",
    "print(inds)\n",
    "print(len(inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baee109c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3739449, 190)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the industrial code to dummies and merge to the dataframe. Keep k-1 ind dummies to avoid multi-colinearity\n",
    "ind_dummies = pd.get_dummies(df['permno'], prefix=\"ind\", drop_first=True)\n",
    "df = pd.concat([df,ind_dummies], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1da932d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>excess_ret</th>\n",
       "      <th>ret</th>\n",
       "      <th>rfree</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>ind_84</th>\n",
       "      <th>ind_85</th>\n",
       "      <th>ind_86</th>\n",
       "      <th>ind_87</th>\n",
       "      <th>ind_88</th>\n",
       "      <th>ind_89</th>\n",
       "      <th>ind_90</th>\n",
       "      <th>ind_91</th>\n",
       "      <th>ind_92</th>\n",
       "      <th>ind_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986-02-01</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.262610</td>\n",
       "      <td>-0.257143</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>-0.375440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>10</td>\n",
       "      <td>0.360335</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>-0.496811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986-04-01</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.103717</td>\n",
       "      <td>-0.098592</td>\n",
       "      <td>0.005125</td>\n",
       "      <td>-0.401783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.521182</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1986-05-01</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.227831</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>-0.435735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.439391</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-06-01</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.009883</td>\n",
       "      <td>-0.005025</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>-0.534203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.499678</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date permno  excess_ret       ret     rfree     mvel1  beta  betasq  \\\n",
       "0 1986-02-01     10   -0.262610 -0.257143  0.005467 -0.375440   0.0     0.0   \n",
       "1 1986-03-01     10    0.360335  0.365385  0.005050 -0.496811   0.0     0.0   \n",
       "2 1986-04-01     10   -0.103717 -0.098592  0.005125 -0.401783   0.0     0.0   \n",
       "3 1986-05-01     10   -0.227831 -0.222656  0.005175 -0.435735   0.0     0.0   \n",
       "4 1986-06-01     10   -0.009883 -0.005025  0.004858 -0.534203   0.0     0.0   \n",
       "\n",
       "   chmom    dolvol  ...  ind_84  ind_85  ind_86  ind_87  ind_88  ind_89  \\\n",
       "0    0.0  0.000000  ...       0       0       0       0       0       0   \n",
       "1    0.0  0.000000  ...       0       0       0       0       0       0   \n",
       "2    0.0 -0.521182  ...       0       0       0       0       0       0   \n",
       "3    0.0 -0.439391  ...       0       0       0       0       0       0   \n",
       "4    0.0 -0.499678  ...       0       0       0       0       0       0   \n",
       "\n",
       "   ind_90  ind_91  ind_92  ind_93  \n",
       "0       0       0       0       0  \n",
       "1       0       0       0       0  \n",
       "2       0       0       0       0  \n",
       "3       0       0       0       0  \n",
       "4       0       0       0       0  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eab8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features (X) + targets (Y)\n",
    "# Note: for X, use everything except return info and IDs\n",
    "#X = df.drop(columns=['ret', 'excess_ret', 'rfree', 'permno', 'date'])\n",
    "X = df.drop(columns=['ret', 'excess_ret', 'rfree', 'permno']) \n",
    "y = df['excess_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468333d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcd60124",
   "metadata": {},
   "source": [
    "### Exploring ways to do time series cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "91be00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_time_series_split(start_year, end_year, train_size, val_size, test_size, step=1):\n",
    "    current_year = start_year\n",
    "    while current_year + train_size + val_size + test_size <= end_year:\n",
    "        \n",
    "        train_start = start_year\n",
    "        train_end = current_year + train_size\n",
    "        \n",
    "        val_start = train_end\n",
    "        val_end = val_start + val_size\n",
    "        \n",
    "        test_start = val_end\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        yield np.arange(train_start, train_end), np.arange(val_start, val_end), np.arange(test_start, test_end)\n",
    "\n",
    "        current_year += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d8f23e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974]\n",
      "Validation year:  index=[1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986]\n",
      "Test year:  index=[1987]\n",
      "\n",
      "Fold 1:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975]\n",
      "Validation year:  index=[1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987]\n",
      "Test year:  index=[1988]\n",
      "\n",
      "Fold 2:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976]\n",
      "Validation year:  index=[1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988]\n",
      "Test year:  index=[1989]\n",
      "\n",
      "Fold 3:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977]\n",
      "Validation year:  index=[1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989]\n",
      "Test year:  index=[1990]\n",
      "\n",
      "Fold 4:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978]\n",
      "Validation year:  index=[1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990]\n",
      "Test year:  index=[1991]\n",
      "\n",
      "Fold 5:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979]\n",
      "Validation year:  index=[1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991]\n",
      "Test year:  index=[1992]\n",
      "\n",
      "Fold 6:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980]\n",
      "Validation year:  index=[1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992]\n",
      "Test year:  index=[1993]\n",
      "\n",
      "Fold 7:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981]\n",
      "Validation year:  index=[1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993]\n",
      "Test year:  index=[1994]\n",
      "\n",
      "Fold 8:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982]\n",
      "Validation year:  index=[1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994]\n",
      "Test year:  index=[1995]\n",
      "\n",
      "Fold 9:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983]\n",
      "Validation year:  index=[1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995]\n",
      "Test year:  index=[1996]\n",
      "\n",
      "Fold 10:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984]\n",
      "Validation year:  index=[1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996]\n",
      "Test year:  index=[1997]\n",
      "\n",
      "Fold 11:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985]\n",
      "Validation year:  index=[1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997]\n",
      "Test year:  index=[1998]\n",
      "\n",
      "Fold 12:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986]\n",
      "Validation year:  index=[1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998]\n",
      "Test year:  index=[1999]\n",
      "\n",
      "Fold 13:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987]\n",
      "Validation year:  index=[1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999]\n",
      "Test year:  index=[2000]\n",
      "\n",
      "Fold 14:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988]\n",
      "Validation year:  index=[1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000]\n",
      "Test year:  index=[2001]\n",
      "\n",
      "Fold 15:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989]\n",
      "Validation year:  index=[1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001]\n",
      "Test year:  index=[2002]\n",
      "\n",
      "Fold 16:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990]\n",
      "Validation year:  index=[1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002]\n",
      "Test year:  index=[2003]\n",
      "\n",
      "Fold 17:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991]\n",
      "Validation year:  index=[1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003]\n",
      "Test year:  index=[2004]\n",
      "\n",
      "Fold 18:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992]\n",
      "Validation year:  index=[1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004]\n",
      "Test year:  index=[2005]\n",
      "\n",
      "Fold 19:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993]\n",
      "Validation year:  index=[1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005]\n",
      "Test year:  index=[2006]\n",
      "\n",
      "Fold 20:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994]\n",
      "Validation year:  index=[1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006]\n",
      "Test year:  index=[2007]\n",
      "\n",
      "Fold 21:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995]\n",
      "Validation year:  index=[1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007]\n",
      "Test year:  index=[2008]\n",
      "\n",
      "Fold 22:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996]\n",
      "Validation year:  index=[1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008]\n",
      "Test year:  index=[2009]\n",
      "\n",
      "Fold 23:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997]\n",
      "Validation year:  index=[1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009]\n",
      "Test year:  index=[2010]\n",
      "\n",
      "Fold 24:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998]\n",
      "Validation year:  index=[1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010]\n",
      "Test year:  index=[2011]\n",
      "\n",
      "Fold 25:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998\n",
      " 1999]\n",
      "Validation year:  index=[2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011]\n",
      "Test year:  index=[2012]\n",
      "\n",
      "Fold 26:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998\n",
      " 1999 2000]\n",
      "Validation year:  index=[2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012]\n",
      "Test year:  index=[2013]\n",
      "\n",
      "Fold 27:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998\n",
      " 1999 2000 2001]\n",
      "Validation year:  index=[2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013]\n",
      "Test year:  index=[2014]\n",
      "\n",
      "Fold 28:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998\n",
      " 1999 2000 2001 2002]\n",
      "Validation year:  index=[2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014]\n",
      "Test year:  index=[2015]\n",
      "\n",
      "Fold 29:\n",
      "Train year: index=[1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n",
      " 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984\n",
      " 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998\n",
      " 1999 2000 2001 2002 2003]\n",
      "Validation year:  index=[2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015]\n",
      "Test year:  index=[2016]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_year = 1957\n",
    "end_year = 2017\n",
    "train_size = 18  # Number of years in the training set\n",
    "val_size = 12    # Number of years in the validation set\n",
    "test_size = 1    # Number of years in the test set\n",
    "\n",
    "for i, (train_year, val_year, test_year) in enumerate(custom_time_series_split(start_year, end_year, train_size, val_size, test_size)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    #train_set = your_data[train_index]\n",
    "    print(f\"Train year: index={train_year}\")\n",
    "    \n",
    "    #val_set = your_data[val_index]\n",
    "    print(f\"Validation year:  index={val_year}\")\n",
    "    \n",
    "    #test_set = your_data[test_index]\n",
    "    print(f\"Test year:  index={test_year}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b1bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b1e2882",
   "metadata": {},
   "source": [
    "**Time series cross validation illustration plot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7591a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tscv_plot(start_year, end_year, train_size, val_size, test_size, step=1):\n",
    "    \n",
    "    # Get all the date indexes for all the folds into a dictionary\n",
    "    folds = []\n",
    "    current_year = start_year\n",
    "    \n",
    "    while current_year + train_size + val_size + test_size <= end_year:\n",
    "        \n",
    "        train_start = start_year\n",
    "        train_end = current_year + train_size\n",
    "        \n",
    "        val_start = train_end\n",
    "        val_end = val_start + val_size\n",
    "        \n",
    "        test_start = val_end\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        date_info = {'train_start':start_year, 'train_end':train_end, 'val_start':val_start, 'val_end':val_end, \n",
    "                     'test_start':test_start, 'test_end':test_end}\n",
    "        folds.append(date_info)\n",
    "        current_year += step\n",
    "    \n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(16,8))\n",
    "\n",
    "    # Set the y-axis ticks and labels\n",
    "    y_ticks = range(1, len(folds) + 1)\n",
    "    y_labels = [f'Fold {i}' for i in y_ticks[::-1]]\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "    # Plot the bars for each fold\n",
    "    i = 1\n",
    "    for fold in folds[::-1]:\n",
    "        train_start = int(fold['train_start'])\n",
    "        train_end = int(fold['train_end'])\n",
    "        val_start = int(fold['val_start'])\n",
    "        val_end = int(fold['val_end'])\n",
    "        test_start = int(fold['test_start'])\n",
    "        test_end = int(fold['test_end'])\n",
    "\n",
    "        ax.barh(i, train_end - train_start, left=train_start, height=0.5, color='blue', label='Train')\n",
    "        ax.barh(i, val_end - val_start, left=val_start, height=0.5, color='orange', label='Validation')\n",
    "        ax.barh(i, test_end - test_start, left=test_start, height=0.5, color='red', label='Test')\n",
    "        i += 1\n",
    "\n",
    "    # Set the x-axis limits and labels\n",
    "    ax.set_xlim(int(folds[0]['train_start']), int(folds[-1]['test_end']))\n",
    "    ax.set_xlabel('Year')\n",
    "\n",
    "    # Add a legend\n",
    "    handles = [plt.Rectangle((0, 0), 1, 1, color='blue'),\n",
    "               plt.Rectangle((0, 0), 1, 1, color='orange'),\n",
    "               plt.Rectangle((0, 0), 1, 1, color='red')]\n",
    "    labels = ['Train', 'Validation', 'Test']\n",
    "    ax.legend(handles, labels, loc='upper right')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "86f99bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_year = 1957\n",
    "end_year = 2017\n",
    "train_size = 18  # Number of years in the training set\n",
    "val_size = 12    # Number of years in the validation set\n",
    "test_size = 1    # Number of years in the test set\n",
    "\n",
    "get_tscv_plot(start_year, end_year, train_size, val_size, test_size, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9cde99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86607ce1",
   "metadata": {},
   "source": [
    "**Modified the above 'custom_time_series_split' function to make it compatible with datetime format:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58365d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ts_split(start_year, end_year, train_size, val_size, test_size, step=1):\n",
    "    \n",
    "    start_year_dt = datetime.strptime(start_year, '%Y-%m')\n",
    "    end_year_dt = datetime.strptime(end_year, '%Y-%m')\n",
    "    train_size_dt = relativedelta(years=train_size)\n",
    "    val_size_dt = relativedelta(years=val_size)\n",
    "    test_size_dt = relativedelta(years=test_size)\n",
    "    step_dt = relativedelta(years=step)\n",
    "    \n",
    "    current_year = start_year_dt\n",
    "    \n",
    "    while current_year + train_size_dt + val_size_dt + test_size_dt <= end_year_dt:\n",
    "        \n",
    "        train_start = start_year_dt\n",
    "        train_end = current_year + train_size_dt\n",
    "        \n",
    "        val_start = train_end\n",
    "        val_end = val_start + val_size_dt\n",
    "        \n",
    "        test_start = val_end\n",
    "        test_end = test_start + test_size_dt\n",
    "\n",
    "        yield train_end, val_end, test_end\n",
    "\n",
    "        current_year += step_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b5ebec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Train end: index=1975-03-01 00:00:00\n",
      "Validation end:  index=1987-03-01 00:00:00\n",
      "Test end:  index=1988-03-01 00:00:00\n",
      "\n",
      "Fold 1:\n",
      "Train end: index=1976-03-01 00:00:00\n",
      "Validation end:  index=1988-03-01 00:00:00\n",
      "Test end:  index=1989-03-01 00:00:00\n",
      "\n",
      "Fold 2:\n",
      "Train end: index=1977-03-01 00:00:00\n",
      "Validation end:  index=1989-03-01 00:00:00\n",
      "Test end:  index=1990-03-01 00:00:00\n",
      "\n",
      "Fold 3:\n",
      "Train end: index=1978-03-01 00:00:00\n",
      "Validation end:  index=1990-03-01 00:00:00\n",
      "Test end:  index=1991-03-01 00:00:00\n",
      "\n",
      "Fold 4:\n",
      "Train end: index=1979-03-01 00:00:00\n",
      "Validation end:  index=1991-03-01 00:00:00\n",
      "Test end:  index=1992-03-01 00:00:00\n",
      "\n",
      "Fold 5:\n",
      "Train end: index=1980-03-01 00:00:00\n",
      "Validation end:  index=1992-03-01 00:00:00\n",
      "Test end:  index=1993-03-01 00:00:00\n",
      "\n",
      "Fold 6:\n",
      "Train end: index=1981-03-01 00:00:00\n",
      "Validation end:  index=1993-03-01 00:00:00\n",
      "Test end:  index=1994-03-01 00:00:00\n",
      "\n",
      "Fold 7:\n",
      "Train end: index=1982-03-01 00:00:00\n",
      "Validation end:  index=1994-03-01 00:00:00\n",
      "Test end:  index=1995-03-01 00:00:00\n",
      "\n",
      "Fold 8:\n",
      "Train end: index=1983-03-01 00:00:00\n",
      "Validation end:  index=1995-03-01 00:00:00\n",
      "Test end:  index=1996-03-01 00:00:00\n",
      "\n",
      "Fold 9:\n",
      "Train end: index=1984-03-01 00:00:00\n",
      "Validation end:  index=1996-03-01 00:00:00\n",
      "Test end:  index=1997-03-01 00:00:00\n",
      "\n",
      "Fold 10:\n",
      "Train end: index=1985-03-01 00:00:00\n",
      "Validation end:  index=1997-03-01 00:00:00\n",
      "Test end:  index=1998-03-01 00:00:00\n",
      "\n",
      "Fold 11:\n",
      "Train end: index=1986-03-01 00:00:00\n",
      "Validation end:  index=1998-03-01 00:00:00\n",
      "Test end:  index=1999-03-01 00:00:00\n",
      "\n",
      "Fold 12:\n",
      "Train end: index=1987-03-01 00:00:00\n",
      "Validation end:  index=1999-03-01 00:00:00\n",
      "Test end:  index=2000-03-01 00:00:00\n",
      "\n",
      "Fold 13:\n",
      "Train end: index=1988-03-01 00:00:00\n",
      "Validation end:  index=2000-03-01 00:00:00\n",
      "Test end:  index=2001-03-01 00:00:00\n",
      "\n",
      "Fold 14:\n",
      "Train end: index=1989-03-01 00:00:00\n",
      "Validation end:  index=2001-03-01 00:00:00\n",
      "Test end:  index=2002-03-01 00:00:00\n",
      "\n",
      "Fold 15:\n",
      "Train end: index=1990-03-01 00:00:00\n",
      "Validation end:  index=2002-03-01 00:00:00\n",
      "Test end:  index=2003-03-01 00:00:00\n",
      "\n",
      "Fold 16:\n",
      "Train end: index=1991-03-01 00:00:00\n",
      "Validation end:  index=2003-03-01 00:00:00\n",
      "Test end:  index=2004-03-01 00:00:00\n",
      "\n",
      "Fold 17:\n",
      "Train end: index=1992-03-01 00:00:00\n",
      "Validation end:  index=2004-03-01 00:00:00\n",
      "Test end:  index=2005-03-01 00:00:00\n",
      "\n",
      "Fold 18:\n",
      "Train end: index=1993-03-01 00:00:00\n",
      "Validation end:  index=2005-03-01 00:00:00\n",
      "Test end:  index=2006-03-01 00:00:00\n",
      "\n",
      "Fold 19:\n",
      "Train end: index=1994-03-01 00:00:00\n",
      "Validation end:  index=2006-03-01 00:00:00\n",
      "Test end:  index=2007-03-01 00:00:00\n",
      "\n",
      "Fold 20:\n",
      "Train end: index=1995-03-01 00:00:00\n",
      "Validation end:  index=2007-03-01 00:00:00\n",
      "Test end:  index=2008-03-01 00:00:00\n",
      "\n",
      "Fold 21:\n",
      "Train end: index=1996-03-01 00:00:00\n",
      "Validation end:  index=2008-03-01 00:00:00\n",
      "Test end:  index=2009-03-01 00:00:00\n",
      "\n",
      "Fold 22:\n",
      "Train end: index=1997-03-01 00:00:00\n",
      "Validation end:  index=2009-03-01 00:00:00\n",
      "Test end:  index=2010-03-01 00:00:00\n",
      "\n",
      "Fold 23:\n",
      "Train end: index=1998-03-01 00:00:00\n",
      "Validation end:  index=2010-03-01 00:00:00\n",
      "Test end:  index=2011-03-01 00:00:00\n",
      "\n",
      "Fold 24:\n",
      "Train end: index=1999-03-01 00:00:00\n",
      "Validation end:  index=2011-03-01 00:00:00\n",
      "Test end:  index=2012-03-01 00:00:00\n",
      "\n",
      "Fold 25:\n",
      "Train end: index=2000-03-01 00:00:00\n",
      "Validation end:  index=2012-03-01 00:00:00\n",
      "Test end:  index=2013-03-01 00:00:00\n",
      "\n",
      "Fold 26:\n",
      "Train end: index=2001-03-01 00:00:00\n",
      "Validation end:  index=2013-03-01 00:00:00\n",
      "Test end:  index=2014-03-01 00:00:00\n",
      "\n",
      "Fold 27:\n",
      "Train end: index=2002-03-01 00:00:00\n",
      "Validation end:  index=2014-03-01 00:00:00\n",
      "Test end:  index=2015-03-01 00:00:00\n",
      "\n",
      "Fold 28:\n",
      "Train end: index=2003-03-01 00:00:00\n",
      "Validation end:  index=2015-03-01 00:00:00\n",
      "Test end:  index=2016-03-01 00:00:00\n",
      "\n",
      "Fold 29:\n",
      "Train end: index=2004-03-01 00:00:00\n",
      "Validation end:  index=2016-03-01 00:00:00\n",
      "Test end:  index=2017-03-01 00:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_year = '1957-03'\n",
    "end_year = '2017-03'\n",
    "train_size = 18  # Number of years in the training set\n",
    "val_size = 12    # Number of years in the validation set\n",
    "test_size = 1    # Number of years in the test set\n",
    "\n",
    "for i, (train_end, val_end, test_end) in enumerate(custom_ts_split(start_year, end_year, train_size, val_size, test_size)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    \n",
    "    #train_set = your_data[train_index]\n",
    "    print(f\"Train end: index={train_end}\")\n",
    "    \n",
    "    #val_set = your_data[val_index]\n",
    "    print(f\"Validation end:  index={val_end}\")\n",
    "    \n",
    "    #test_set = your_data[test_index]\n",
    "    print(f\"Test end:  index={test_end}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd182e56",
   "metadata": {},
   "source": [
    "**Potential code to tune hyperparameters if grid search unable to divide train, validation and test set as desired:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58857176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with Fold 0...\n",
      "Working with Fold 1...\n"
     ]
    }
   ],
   "source": [
    "start_year = '1957-03'\n",
    "end_year = '1989-03'\n",
    "train_size = 18        # Number of years in the training set\n",
    "val_size = 12          # Number of years in the validation set\n",
    "test_size = 1          # Number of years in the test set\n",
    "\n",
    "date = df[\"date\"]\n",
    "\n",
    "for i, (train_end, val_end, test_end) in enumerate(custom_ts_split(start_year, end_year, train_size, val_size, test_size)):\n",
    "    \n",
    "    print(f\"Working with Fold {i}...\")\n",
    "    \n",
    "    # Define training set\n",
    "    training = (date <= train_end)\n",
    "    X_train, y_train = X.loc[training], y.loc[training]\n",
    "    \n",
    "    # Define validation set\n",
    "    validation = (date > train_end) & (date <= val_end) \n",
    "    X_val, y_val = X.loc[validation], y.loc[validation]\n",
    "    \n",
    "    # Define test set\n",
    "    test = (date > val_end) & (date <= test_end)\n",
    "    X_test, y_test = X.loc[test], y.loc[test]\n",
    "    \n",
    "    #for param1 in hyperparam_grid:\n",
    "        #for param2 in hyperparam_grid:\n",
    "            # Fit this set of params to NN2. E.g. model = fit_nn2(param1, param2, ..., X_train, y_train, X_val, y_val)\n",
    "            # record hyperparam set + performance\n",
    "    \n",
    "    # Pick a hyperparam set with the best performance\n",
    "    # Train this best model on (train + validation) set\n",
    "    # Evaluate the model on test set, setting the performance metrix to R^2 OOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252010e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f781e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d485c759",
   "metadata": {},
   "source": [
    "### Functions to create models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5759bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn2():\n",
    "    \n",
    "    # model architechture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add layers\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "010bd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn3():\n",
    "    \n",
    "    # model architechture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add layers\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf54d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn4():\n",
    "    \n",
    "    # model architechture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add layers\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214472d9",
   "metadata": {},
   "source": [
    "### Functions to fit models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39cab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn2(X_train, y_train, X_val, y_val, epochs, batch_size):\n",
    "    \n",
    "    # model architechture\n",
    "    model = Sequential()\n",
    "    \n",
    "    # add layers\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "    # fit the model\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size, \n",
    "                        verbose=False,\n",
    "                        validation_data = (X_val, y_val))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c1fb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26be0fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a51a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7e2124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63fa7a3c",
   "metadata": {},
   "source": [
    "**Example of performing grid search on JUST ONE set of training and validating data**\n",
    "<br> Can Ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9cfe3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features (X) + targets (Y)\n",
    "# Note: for X, use everything except return info and IDs\n",
    "X = df.drop(columns=['ret', 'excess_ret', 'rfree', 'permno', 'date']) \n",
    "y = df['excess_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dadd2801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3739449, 185) (3739449,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fcc8a196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: below represents just ONE set of training and testing within an expanding window time-series cross validation\n",
    "# e.g. make 4 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1975-03') # selects \n",
    "X_train, y_train = X.loc[training], y.loc[training]\n",
    "\n",
    "# e.g. make 2 years of validation data\n",
    "validation = (date > '1975-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation], y.loc[validation]\n",
    "\n",
    "# make test data (ignore for now)\n",
    "test = (date > '1987-03') & (date <= '1988-03')\n",
    "X_test, y_test = X.loc[test], y.loc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1a66700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1988-03-01 00:00:00')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc91acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: below represents just ONE set of training and testing within an expanding window time-series cross validation\n",
    "# e.g. make 4 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1975-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values\n",
    "\n",
    "# e.g. make 2 years of validation data\n",
    "validation = (date > '1975-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation].values, y.loc[validation].values \n",
    "\n",
    "# make test data (ignore for now)\n",
    "test = (date > '1987-03') & (date <= '1988-03')\n",
    "X_test, y_test = X.loc[test].values, y.loc[test].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bec733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and wrap it in a KerasRegressor for compatibility with scikit-learn\n",
    "model = KerasRegressor(model=create_nn2, verbose=0)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "batch_size = [10, 20, 40]\n",
    "epochs = [10, 20, 30]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=2)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb07cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary dataframe showing all combinations\n",
    "df = pd.DataFrame(grid_result.cv_results_)\n",
    "df[['param_batch_size', 'param_epochs', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store best model\n",
    "best_model = grid_result.best_estimator_\n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "display(best_model, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model using the training data\n",
    "best_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n",
    "\n",
    "# Evaluate it on validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "score = mean_squared_error(y_val, y_pred)\n",
    "print(f\"Evaluation score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model using training set and evaluate it on validation set\n",
    "#fit_nn2(X_train, y_train, X_val, y_val, epochs=best_params['epochs'], batch_size=best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea02c7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af315654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250ccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659aac3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d7c513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f799cd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
